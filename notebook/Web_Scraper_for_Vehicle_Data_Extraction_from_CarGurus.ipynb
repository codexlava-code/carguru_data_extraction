{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "We are looking for a skilled web scraper to extract vehicle listings for a specific dealership from the CarGurus website. The ideal candidate will have experience in web scraping and data extraction, ensuring accuracy and efficiency. The project involves retrieving details such as make, model, price, and mileage. The scraped data will be used for analysis and reporting purposes.\n",
    "\n",
    "given this dealership url:\n",
    "https://www.cargurus.com/Cars/m-Carrio-MotorCars-sp385771\n",
    "\n",
    "Symbol Instruction:\n",
    "- ‚¨úÔ∏è Task added\n",
    "- ‚úÖ Task Completed\n",
    "- üü© Task Progress\n",
    "- üü• Task Issue\n",
    "- üü¶ Task Plan\n",
    "\n",
    "**Following Task:**\n",
    "1. ‚¨úÔ∏è Mock dealerships API endpoint\n",
    "2. ‚¨úÔ∏è Navigate to Dealership's Home page\n",
    "3. ‚¨úÔ∏è Extract Dealership details\n",
    "4. ‚¨úÔ∏è Navigate through ALL of a dealership's vehicles extract links\n",
    "5. ‚¨úÔ∏è Extract a Vehicle's details\n",
    "6. ‚¨úÔ∏è trigger extraction jobs for all dealerships\n",
    "7. ‚¨úÔ∏è Design API endpoint to post vehicle data\n",
    "8. ‚¨úÔ∏è Database writes (preventing duplication)\n",
    "9. ‚¨úÔ∏è Additional Retry attempts (2)\n",
    "10. ‚¨úÔ∏è Scraper Error Handling by slack post\n",
    "11. ‚¨úÔ∏è Scraper Cron Job implementation\n",
    "12. ‚¨úÔ∏è Deploy Scraper to server to digital Ocean\n",
    "\n",
    "\n",
    "#### Package installation command:\n",
    "`!pip install lxml pandas slack_sdk selenium schedule webdriver-manager`"
   ],
   "metadata": {
    "id": "AwfBHX3Ugvg4"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5mzORnzSSbpD",
    "ExecuteTime": {
     "end_time": "2025-04-01T11:25:32.479622Z",
     "start_time": "2025-04-01T11:25:32.475886Z"
    }
   },
   "source": [
    "import requests, urllib.parse, lxml, pandas as pd, time, requests, json, os, schedule, hashlib, hmac\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:12.692624Z",
     "start_time": "2025-03-27T14:26:08.699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scroll_incrementally(driver, pause_time=2, max_scroll_attempts=3):\n",
    "    \"\"\"Scroll incrementally and wait for content to load.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for attempt in range(max_scroll_attempts):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_time)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:  # Exit if no new content is loaded\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "############################ For Linux\n",
    "# def initialize_driver():\n",
    "#     \"\"\"Initializes and returns a cross-platform Selenium WebDriver.\"\"\"\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#\n",
    "#     # Add headless mode only if running on a server\n",
    "#     options.add_argument(\"--headless\")  # Runs without UI\n",
    "#     options.add_argument(\"--no-sandbox\")  # Required for Linux servers\n",
    "#     options.add_argument(\"--disable-dev-shm-usage\")  # Prevent memory issues\n",
    "#     options.add_argument(\"--disable-gpu\")  # Fixes rendering issues in headless mode\n",
    "#     options.add_argument(\"--remote-debugging-port=9222\")  # Debugging\n",
    "#\n",
    "#     return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "##################### For WIndows\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a Selenium WebDriver.\"\"\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run without UI\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--disable-application-cache\")  # Disable caching\n",
    "    options.add_argument(\"--incognito\")  # Use incognito mode\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_page_load_timeout(500)  # Set timeout for page loading\n",
    "    return driver\n",
    "\n",
    "# Create a new driver instance for each scraping attempt\n",
    "driver = initialize_driver()\n",
    "scroll_incrementally(driver)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TASK 10. Scraper error handling and post on slack by integration"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:16.608236Z",
     "start_time": "2025-03-27T14:26:16.600684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# up next:\n",
    "# put up on accessible endpoint as a stand-alone micro service?\n",
    "SLACK_BOT_TOKEN=\"xoxb-8104347625139-8544713643409-37Z4UzEIkGpUA5ArzSUx3Wc6\"\n",
    "SLACK_SIGNING_SECRET=\"ecdfd545eac42f98586bb00519ec5df7\"\n",
    "\n",
    "class SlackClient:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Slack client with bot token and signing secret.\"\"\"\n",
    "        print(\"Initializing Slack client with token from environment...\")\n",
    "        self.token = SLACK_BOT_TOKEN\n",
    "        if not self.token:\n",
    "            raise ValueError(\"SLACK_BOT_TOKEN not found in environment variables\")\n",
    "        self.client = WebClient(token=self.token)\n",
    "        self.signing_secret = SLACK_SIGNING_SECRET\n",
    "\n",
    "    def send_message(self, message: str, channel_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Send a message to a Slack channel.\n",
    "\n",
    "        Args:\n",
    "            message (str): The message text to send\n",
    "            channel_id (str): The ID of the channel to send the message to\n",
    "\n",
    "        Returns:\n",
    "            bool: True if message was sent successfully, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat_postMessage(\n",
    "                channel=channel_id,\n",
    "                text=message\n",
    "            )\n",
    "            return True\n",
    "        except SlackApiError as e:\n",
    "            error_details = {\n",
    "                'error': str(e.response['error']),\n",
    "                'response': e.response.data,\n",
    "                'status_code': e.response.status_code,\n",
    "                'headers': dict(e.response.headers)\n",
    "            }\n",
    "            print(\"Detailed Slack Error:\")\n",
    "            print(f\"Error Type: {error_details['error']}\")\n",
    "            print(f\"Status Code: {error_details['status_code']}\")\n",
    "            print(f\"Response Data: {error_details['response']}\")\n",
    "            print(f\"Headers: {error_details['headers']}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    # Ensures that incoming requests to your microservice actually originate from Slack and haven‚Äôt been tampered with.\n",
    "    # This is critical for security when handling webhooks or API calls from Slack.\n",
    "    def verify_slack_request(self, timestamp: str, signature: str, body: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verify that the request actually came from Slack.\n",
    "\n",
    "        Args:\n",
    "            timestamp (str): X-Slack-Request-Timestamp header\n",
    "            signature (str): X-Slack-Signature header\n",
    "            body (str): Raw request body\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the request is valid, False otherwise\n",
    "        \"\"\"\n",
    "        if not self.signing_secret:\n",
    "            print(\"Warning: SLACK_SIGNING_SECRET not set\")\n",
    "            return False\n",
    "\n",
    "        # Check if the timestamp is too old\n",
    "        if abs(time.time() - int(timestamp)) > 60 * 5:\n",
    "            return False\n",
    "\n",
    "        # Create the signature base string\n",
    "        sig_basestring = f\"v0:{timestamp}:{body}\"\n",
    "\n",
    "        # Calculate the signature\n",
    "        my_signature = 'v0=' + hmac.new(\n",
    "            self.signing_secret.encode(),\n",
    "            sig_basestring.encode(),\n",
    "            hashlib.sha256\n",
    "        ).hexdigest()\n",
    "\n",
    "        # Compare signatures\n",
    "        return hmac.compare_digest(my_signature, signature)\n",
    "\n",
    "# Initialize the Slack client\n",
    "slack_obj = SlackClient()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Slack client with token from environment...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TASK 1. Mock dealerships API endpoint"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:21.928682Z",
     "start_time": "2025-03-27T14:26:21.924939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dealerships_api_url = \"https://api/v1/scraper/dealership-marketplaces\"\n",
    "# headers = {\"accept\": \"application/json\"}\n",
    "# response = requests.get(url, headers=headers)\n",
    "# print(response.json())\n",
    "\n",
    "dealerships_api_url = [\n",
    "    # {\n",
    "    #   \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n",
    "    #   \"address_id\": \"223e4567-e89b-12d3-a456-426614174001\",\n",
    "    #   \"inventory_source_id\": \"323e4567-e89b-12d3-a456-426614174002\",\n",
    "    #   \"name\": \"Best Dealership\",\n",
    "    #   \"phone_number\": \"555-123-4567\",\n",
    "    #   \"email\": \"contact@bestdealership.com\",\n",
    "    #   \"general_manager\": \"John Doe\",\n",
    "    #   \"website\": \"http://www.bestdealership.com\",\n",
    "    #   \"created_at\": \"2021-01-01T12:00:00Z\",\n",
    "    #   \"updated_at\": \"2021-01-02T12:00:00Z\",\n",
    "    #   \"inventory_source\": {\n",
    "    #     \"id\": \"323e4567-e89b-12d3-a456-426614174002\",\n",
    "    #     \"url\": \"https://www.cargurus.com/Cars/m-Twins-Auto-Sales--Taylor-sp457133\",\n",
    "    #     \"category\": \"car_gurus\",\n",
    "    #     \"created_at\": \"2021-01-01T12:00:00Z\",\n",
    "    #     \"updated_at\": \"2021-01-02T12:00:00Z\"\n",
    "    #   }\n",
    "    # },\n",
    "    {\n",
    "      \"id\": \"423e4567-e89b-12d3-a456-426614174003\",\n",
    "      \"address_id\": \"523e4567-e89b-12d3-a456-426614174004\",\n",
    "      \"inventory_source_id\": \"623e4567-e89b-12d3-a456-426614174005\",\n",
    "      \"name\": \"Quality Cars\",\n",
    "      \"phone_number\": \"555-987-6543\",\n",
    "      \"email\": \"info@qualitycars.com\",\n",
    "      \"general_manager\": \"Jane Smith\",\n",
    "      \"website\": \"http://www.qualitycars.com\",\n",
    "      \"created_at\": \"2021-02-01T12:00:00Z\",\n",
    "      \"updated_at\": \"2021-02-02T12:00:00Z\",\n",
    "      \"inventory_source\": {\n",
    "        \"id\": \"623e4567-e89b-12d3-a456-426614174005\",\n",
    "        \"url\": \"https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683\", #  https://www.cargurus.com/Cars/m-Carrio-MotorCars-sp385771\n",
    "        \"category\": \"car_gurus\",\n",
    "        \"created_at\": \"2021-02-01T12:00:00Z\",\n",
    "        \"updated_at\": \"2021-02-02T12:00:00Z\"\n",
    "      }\n",
    "    }\n",
    "  ]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### TASK 2. Navigate to dealership home page and\n",
    "### TASK 3. extract dealership details\n",
    "**extract a dealership‚Äôs details:**\n",
    "- address\n",
    "- email\n",
    "- hours of operation\n",
    "- phone number\n",
    "- description"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:32.561048Z",
     "start_time": "2025-03-27T14:26:28.711083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_dealership_data(url, dealership_name):\n",
    "    driver.get(url)\n",
    "    soup_data = BeautifulSoup(driver.page_source, \"lxml\") #response.content\n",
    "    # print(soup_data.prettify())\n",
    "    try:\n",
    "        title = soup_data.find(\"div\", class_=\"dealerDetailsHeader\").find(\"h1\", class_=\"dealerName\").get_text(strip=True) if \\\n",
    "            (soup_data.find(\"div\", class_=\"dealerDetailsHeader\").find(\"h1\", class_=\"dealerName\")) else None\n",
    "        address = ' '.join(soup_data.find('div', class_='dealerDetailsInfo').find_all(string=True, recursive=False)).strip() if \\\n",
    "            (soup_data.find('div', class_='dealerDetailsInfo').find_all(string=True, recursive=False)) else None\n",
    "        link = soup_data.find(\"p\", class_=\"dealerWebLinks\").find(\"a\").get_text(strip=True) if \\\n",
    "            (soup_data.find(\"p\", class_=\"dealerWebLinks\").find(\"a\")) else None\n",
    "        phone = soup_data.find(\"span\", class_=\"dealerSalesPhone\").get_text(strip=True) if \\\n",
    "            (soup_data.find(\"span\", class_=\"dealerSalesPhone\")) else None\n",
    "        hours_operation = soup_data.find(\"div\", class_=\"dealerText\").get_text(strip=True) if \\\n",
    "            (soup_data.find(\"div\", class_=\"dealerText\")) else None\n",
    "        logo = soup_data.find(\"div\", class_=\"dealerLogo\").find(\"img\").get(\"src\") if \\\n",
    "            (soup_data.find(\"div\", class_=\"dealerLogo\").find(\"img\")) else None\n",
    "\n",
    "        data = {\n",
    "            \"title\": title,\n",
    "            \"link\": link,\n",
    "            \"address\": address,\n",
    "            \"phone\": phone,\n",
    "            \"hours_operation\": hours_operation,\n",
    "            \"logo\": logo,\n",
    "        }\n",
    "        # slack_obj.send_message(\n",
    "        #     message=f\"Success‚úÖ: extracting Dealership details NAME: {dealership_name}, URL: {url}, TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        #     channel_id=\"C08FQ51LEAF\"\n",
    "        # )\n",
    "        return data\n",
    "    except Exception as e :\n",
    "        slack_obj.send_message(\n",
    "            message=f\"Error‚ùå: extracting Dealership details NAME: {dealership_name}, URL: {url}, TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, ERROR: {str(e)}\",\n",
    "            channel_id=\"C08FQ51LEAF\"\n",
    "        )\n",
    "        return print(f\"Error extracting Dealership details for URL {url}: {str(e)}\")\n",
    "\n",
    "\n",
    "dealership_details = [] # Extract dealership details\n",
    "dealership_list = [] # Extract dealership list\n",
    "\n",
    "for dealership in dealerships_api_url:\n",
    "    inventory_source = dealership[\"inventory_source\"]\n",
    "    dealership_id = dealership[\"id\"]\n",
    "    dealership_name = dealership[\"name\"]\n",
    "    inventory_source_id = dealership[\"inventory_source_id\"]\n",
    "    url = inventory_source[\"url\"]\n",
    "    category = inventory_source[\"category\"]\n",
    "    dealership_list.append({\n",
    "        \"dealership_id\": dealership_id,\n",
    "        \"inventory_source_id\": inventory_source_id,\n",
    "        \"dealership_name\": dealership_name,\n",
    "        \"url\": url,\n",
    "        \"category\": category\n",
    "    })\n",
    "\n",
    "    # Extract dealership data\n",
    "    extracted_data = extract_dealership_data(url, dealership_name)\n",
    "    if not extracted_data:\n",
    "        continue  # Skip this entry and move to the next one\n",
    "\n",
    "    dealership_details.append({\n",
    "        \"dealership_name\": dealership_name,\n",
    "        \"data\": extracted_data\n",
    "    })\n",
    "\n",
    "# Final Output: Print the collection of dealership data\n",
    "# for detail in dealership_details:\n",
    "#     print(f\"Dealership: {detail['category']}, Data: {detail['data']}\")\n",
    "print(len(dealership_list))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Navigate through ALL of a dealership's vehicles extract links\n",
    "given a dealership url, navigate through all the vehicles on each page, and all the pages for the dealership."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:44.961440Z",
     "start_time": "2025-03-27T14:26:41.997435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_vehicle_links(soup, base_url):\n",
    "    \"\"\"Extracts vehicle links from the BeautifulSoup object\"\"\"\n",
    "    vehicle_links = []\n",
    "    for a_tag in soup.find_all('a', {'data-testid': 'car-blade-link'}):\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            href = a_tag.get('href')\n",
    "            vehicle_links.append(urllib.parse.urljoin(base_url, href))\n",
    "    return vehicle_links\n",
    "\n",
    "\n",
    "def get_total_pages(soup):\n",
    "    \"\"\"Extracts total number of pages from 'Page X of Y' text.\"\"\"\n",
    "    try:\n",
    "        span_text = soup.find(\"span\", string=lambda text: text and \"Page\" in text)\n",
    "        if span_text:\n",
    "            parts = span_text.text.split()\n",
    "            if len(parts) >= 4 and parts[-1].isdigit():\n",
    "                return int(parts[-1])  # Extract last number (total pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages:{e}\")\n",
    "    return 1  # Default to 1 if not found\n",
    "\n",
    "\n",
    "def get_all_pages(dealership_list):\n",
    "    \"\"\"Scrapes vehicle links from multiple pages by modifying the URL.\"\"\"\n",
    "    increment = 1\n",
    "    for dealership_item in dealership_list:\n",
    "        # Skip specific iteration\n",
    "        # if increment == 1:\n",
    "        #     increment += 1\n",
    "        #     continue\n",
    "        # Stop the loop when increment > 2\n",
    "        # if increment > 1:\n",
    "        #     break\n",
    "        try:\n",
    "            base_url = dealership_item[\"url\"]\n",
    "            driver.get(base_url)\n",
    "            soup_data = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "            # Get Maximum page number\n",
    "            max_pages = get_total_pages(soup_data)\n",
    "            print(f\"Detected {max_pages} total pages.\")\n",
    "            if not max_pages:\n",
    "                continue # Skip this entry and move to the next one\n",
    "\n",
    "            all_vehicle_links = []\n",
    "            for page_num in range(1, max_pages + 1):\n",
    "                page_url = f\"{base_url}#resultsPage={page_num}\"\n",
    "\n",
    "                try:\n",
    "                    driver.get(page_url)\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                    )  # Wait until the body tag is loaded\n",
    "\n",
    "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    vehicle_links = extract_vehicle_links(soup, base_url)\n",
    "\n",
    "                    if not vehicle_links:\n",
    "                        print(\"No vehicle links found on this page. Stopping...\")\n",
    "                        # slack_obj.send_message(\n",
    "                        #     message=f\"Error: No vehicle links found on this page. Stopping...\",\n",
    "                        #     channel_id=\"C08FQ51LEAF\"\n",
    "                        # )\n",
    "                        break\n",
    "\n",
    "                    all_vehicle_links.extend(vehicle_links)\n",
    "                    print(f\"Found {len(vehicle_links)} links on Page {page_num}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    slack_obj.send_message(\n",
    "                        message=f\"Error‚ùå: \"\n",
    "                                f\"extracting Vehicle Navigation URL, \"\n",
    "                                f\"NAME: {dealership_item['dealership_name']}, \"\n",
    "                                f\"URL: {page_url}, \"\n",
    "                                f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, \"\n",
    "                                f\"ERROR: {str(e)\n",
    "                                }\",\n",
    "                        channel_id=\"C08FQ51LEAF\"\n",
    "                    )\n",
    "                    print(f\"error pagination extraction data: {e}\")\n",
    "\n",
    "            dealership_item[\"vehicle_url\"] = all_vehicle_links\n",
    "            # slack_obj.send_message(\n",
    "            #     message=f\"Success‚úÖ: \"\n",
    "            #             f\"Dealership vehicles links: {len(dealership_item[\"vehicle_url\"])}, \"\n",
    "            #             f\"NAME: {dealership_item['dealership_name']},\"\n",
    "            #             f\" URL: {url}, \"\n",
    "            #             f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            #             }\",\n",
    "            #     channel_id=\"C08FQ51LEAF\"\n",
    "            # )\n",
    "            # increment += 1\n",
    "        except Exception as e:\n",
    "            slack_obj.send_message(\n",
    "                message=f\"Error‚ùå: \"\n",
    "                        f\"extracting Dealership navigation, \"\n",
    "                        f\"NAME: {dealership_item['dealership_name']}, \"\n",
    "                        f\"URL: {dealership_item['url']}, \"\n",
    "                        f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, \"\n",
    "                        f\"ERROR: {str(e)\n",
    "                        }\",\n",
    "                channel_id=\"C08FQ51LEAF\"\n",
    "            )\n",
    "            print(f\"error dealership pagination data: {e}\")\n",
    "\n",
    "\n",
    "    return dealership_list\n",
    "\n",
    "# # testing navigation list\n",
    "# dealership_list_demo = [\n",
    "#     {'dealership_id': '123e4567-e89b-12d3-a456-426614174000', 'inventory_source_id': '323e4567-e89b-12d3-a456-426614174002', 'dealership_name': 'Best Dealership', 'url': 'https://www.cargurus.com/Cars/m-Twins-Auto-Sales--Taylor-sp457133', 'category': 'car_gurus'},\n",
    "#     # {'dealership_id': '423e4567-e89b-12d3-a456-426614174003', 'inventory_source_id': '623e4567-e89b-12d3-a456-426614174005', 'dealership_name': 'Quality Cars', 'url': 'https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683', 'category': 'car_gurus'}\n",
    "# ]\n",
    "vehicle_urls = get_all_pages(dealership_list)\n",
    "# print(f\"Total dealership found: {len(vehicle_urls)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 total pages.\n",
      "Found 13 links on Page 1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Extract a Vehicle's details\n",
    "given a vehicle, get all the required data as per our db schema.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:52.664685Z",
     "start_time": "2025-03-27T14:26:52.658107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_vehicle_data(soup_data, vehicle_url, dealership_item):\n",
    "    \"\"\" Extract vehicle data from details page\"\"\"\n",
    "    # title = soup_data.find(\"div\", class_=\"_titleInfo_uw1k0_49\").find(\"h4\").get_text(strip=True) if \\\n",
    "    #     (soup_data.find(\"div\", class_=\"_titleInfo_uw1k0_49\").find(\"h4\")) else None\n",
    "\n",
    "    # price\n",
    "    price_section = soup_data.find(\"div\", class_=\"_dealInfo_uw1k0_70\")\n",
    "    price = None  # Default to None if price is not found\n",
    "    if price_section and price_section.find(\"h5\", class_=\"WoAzt\"):\n",
    "        price = price_section.find(\"h5\", class_=\"WoAzt\").get_text(strip=True).replace(\"$\", \"\").replace(\",\", \"\")\n",
    "\n",
    "\n",
    "    # Features data\n",
    "    features_data = soup_data.find_all(\"li\", class_=\"_listItem_1tanl_14\") or []\n",
    "    # Dictionary to store extracted data\n",
    "    feature_details = {}\n",
    "    for data in features_data:\n",
    "        key = None\n",
    "        value = None\n",
    "        if data.find(\"h5\"):  # Ensure the <h5> tag exists\n",
    "            key = data.find(\"h5\").get_text(strip=True).replace(\" \", \"_\").lower()\n",
    "        if data.find(\"p\"):  # Ensure the <p> tag exists\n",
    "            value = data.find(\"p\").get_text(strip=True)\n",
    "        if key and value:\n",
    "            feature_details[key] = value\n",
    "\n",
    "\n",
    "    # # # extract dl data to dt\n",
    "    records_container = soup_data.find(\"div\", class_=\"_records_1vyus_9\", attrs={\"data-cg-ft\": \"listing-vdp-stats\"})\n",
    "    dt_tags = records_container.find(\"ul\").find_all(\"li\") if (\n",
    "                records_container and records_container.find(\"ul\")) else []\n",
    "\n",
    "    overview_details = {}\n",
    "    for dt_tag in dt_tags:\n",
    "        key = None\n",
    "        value = None\n",
    "        if dt_tag.find(\"span\", class_=\"_label_zbkq7_7\"):  # Check if the key exists\n",
    "            key = (\n",
    "                dt_tag.find(\"span\", class_=\"_label_zbkq7_7\")\n",
    "                .get_text(strip=True)\n",
    "                .replace(\":\", \"\")\n",
    "                .replace(\" \", \"_\")\n",
    "                .lower()\n",
    "            )\n",
    "        if dt_tag.find(\"span\", class_=\"_value_zbkq7_14\"):  # Check if the value exists\n",
    "            value = dt_tag.find(\"span\", class_=\"_value_zbkq7_14\").get_text(strip=True)\n",
    "        if key and value:\n",
    "            overview_details[key] = value\n",
    "\n",
    "\n",
    "     # Parse and build the data dictionary\n",
    "    try:\n",
    "        data = {\n",
    "            \"inventory_source_id\": dealership_item[\"inventory_source_id\"],\n",
    "            \"listing_url\": vehicle_url,\n",
    "            \"status\": \"available\",  # Check availability here if needed\n",
    "            \"price\": float(price) if price else None,  # Gracefully handle None price\n",
    "            \"vehicle\": {\n",
    "                \"dealership_id\": dealership_item[\"dealership_id\"],\n",
    "                \"vin\": overview_details.get(\"vin\"),\n",
    "                \"mileage\": int(feature_details.get(\"mileage\").replace(\",\", \"\")) if feature_details.get(\n",
    "                    \"mileage\") else None,\n",
    "                \"stock_number\": overview_details.get(\"stock_number\"),\n",
    "                \"description\": \"\",\n",
    "                \"exterior_color\": overview_details.get(\"exterior_color\"),\n",
    "                \"interior_color\": overview_details.get(\"interior_color\"),\n",
    "                \"model\": {\n",
    "                    \"name\": overview_details.get(\"model\"),\n",
    "                    \"year\": overview_details.get(\"year\"),\n",
    "                    \"trim\": overview_details.get(\"trim\"),\n",
    "                    \"body_style\": overview_details.get(\"body_type\"),\n",
    "                    \"transmission\": feature_details.get(\"transmission\"),\n",
    "                    \"fuel_type\": feature_details.get(\"fuel_type\"),\n",
    "                    \"drivetrain\": feature_details.get(\"drivetrain\"),\n",
    "                    \"engine\": feature_details.get(\"engine\"),\n",
    "                    \"make\": {\n",
    "                        \"name\": overview_details.get(\"make\"),\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Data construction error for URL {vehicle_url}: {str(e)}\")  # Log the error\n",
    "        data = None  # If any critical data is missing, return None or handle appropriately\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:29:20.034112Z",
     "start_time": "2025-03-27T14:27:08.895894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from random import randint\n",
    "\n",
    "vehicle_output_data = []\n",
    "max_threads = 20  # Adjust this based on your system's capability\n",
    "increment = 1\n",
    "\n",
    "\n",
    "# Function to process individual vehicle URLs (used in parallel threads)\n",
    "def process_vehicle_url(vehicle_url, dealership_item):\n",
    "    driver = None\n",
    "    try:\n",
    "        # print(f\"Start time{increment}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        driver = initialize_driver()  # Create a new WebDriver instance\n",
    "\n",
    "        # Introduce a random delay before loading the page\n",
    "        time.sleep(randint(*(2,5)))\n",
    "\n",
    "        # Load the page\n",
    "        driver.get(vehicle_url)\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.any_of(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"_dealInfo_uw1k0_70\")),\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"_listItem_1tanl_14\")),\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"_records_1vyus_9\")),\n",
    "            )\n",
    "        )\n",
    "        soup_data = BeautifulSoup(driver.page_source, \"lxml\")  # Parse the page source\n",
    "\n",
    "        # Extract vehicle data\n",
    "        vehicle_data = extract_vehicle_data(soup_data, vehicle_url, dealership_item)\n",
    "\n",
    "        if vehicle_data:\n",
    "            # print(f\"end time{increment}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            return vehicle_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for vehicle URL {vehicle_url}: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()  # Close the WebDriver instance\n",
    "    return None\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel scraping\n",
    "for dealership_item in dealership_list:\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Submit scraping tasks for each vehicle URL\n",
    "        futures = {\n",
    "            executor.submit(process_vehicle_url, vehicle_url, dealership_item): vehicle_url\n",
    "            for vehicle_url in dealership_item[\"vehicle_url\"]\n",
    "        }\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            # if increment > 2:\n",
    "            #     break\n",
    "            vehicle_url = futures[future]\n",
    "            try:\n",
    "                result = future.result()  # Get the result of the scraping function\n",
    "                if result:\n",
    "                    vehicle_output_data.append(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {vehicle_url}: {str(e)}\")\n",
    "            # increment += 1\n",
    "\n",
    "\n",
    "print(len(vehicle_output_data))\n",
    "# print(json.dumps(vehicle_output_data, indent=1))\n",
    "# store in a .csv file\n",
    "df = pd.DataFrame(vehicle_output_data)\n",
    "df.to_csv(\"cargurus_cars_data.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for vehicle URL https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=405529067/NONE/DEFAULT: HTTPConnectionPool(host='localhost', port=63561): Read timed out. (read timeout=120)\n",
      "12\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Design API endpoint to post and get vehicle data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:29:53.842042Z",
     "start_time": "2025-03-27T14:29:53.836591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def post_vehicle_data(post_url, data_batch):\n",
    "    # print(f\"Running scheduled task at {datetime.now()}\")\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.post(post_url, data=json.dumps(data_batch), headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        print(f\"Successfully posted {len(data_batch)} records. Status: {response.status_code}\")\n",
    "        # slack_obj.send_message(\n",
    "        #     message=f\"Success‚úÖ: \"\n",
    "        #             f\"Successfully posted Vehicle data {len(data_batch)} records.\"\n",
    "        #             f\" URL: {post_url}, \"\n",
    "        #             f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        #             }\",\n",
    "        #     channel_id=\"C08FQ51LEAF\"\n",
    "        # )\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as errr:\n",
    "        print(f\"Error Send vehicle data: {errr}\")\n",
    "        slack_obj.send_message(\n",
    "            message=f\"Error‚ùå: \"\n",
    "                    f\"Error posted Vehicle data {len(data_batch)} records. \"\n",
    "                    f\"URL: {post_url}, \"\n",
    "                    f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, \"\n",
    "                    f\"ERROR: {str(errr)\n",
    "                    }\",\n",
    "            channel_id=\"C08FQ51LEAF\"\n",
    "        )\n",
    "        return False\n",
    "\n",
    "def get_vehicle_data(get_url):\n",
    "    # print(f\"Running scheduled task at {datetime.now()}\")\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.get(get_url, headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        print(f\"Successfully get records. Status: {response.status_code}\")\n",
    "        slack_obj.send_message(\n",
    "            message=f\"Success‚úÖ: \"\n",
    "                    f\"Successfully posted Vehicle data {len(response.json())} records.\"\n",
    "                    f\" URL: {get_url}, \"\n",
    "                    f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\",\n",
    "            channel_id=\"C08FQ51LEAF\"\n",
    "        )\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as errr:\n",
    "        print(f\"Error posting data: {e}\")\n",
    "        slack_obj.send_message(\n",
    "            message=f\"Error‚ùå: \"\n",
    "                    f\"Error get Vehicle data\"\n",
    "                    f\"URL: {get_url}, \"\n",
    "                    f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, \"\n",
    "                    f\"ERROR: {str(errr)\n",
    "                    }\",\n",
    "            channel_id=\"C08FQ51LEAF\"\n",
    "        )\n",
    "        return []"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Database writes (preventing duplication)\n",
    "\n",
    "**Problem statement**\n",
    "\n",
    "once we‚Äôve extracted info for vehicles, we‚Äôll need to store it in our database. there may already be entries for a given vehicle, if that is the case, overwrite with the latest data.\n",
    "\n",
    "we‚Äôll need to make sure data is always kept up-to-date. and that we have what is truly available. we‚Äôll also need to mark previous vehicles that are no longer part of the inventory as `unavailable`.\n",
    "\n",
    "**Success criteria**\n",
    "\n",
    "- have a way to identify each dealership‚Äôs vehicle. could be the url\n",
    "- save all new vehicles into the database\n",
    "- overwrite existing vehicles‚Äô data to have them show the latest\n",
    "- mark all other vehicles as `unavaliable`\n",
    "- notify us of any failed scrape attempts via `paper boy`"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:30:00.083562Z",
     "start_time": "2025-03-27T14:30:00.079383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_duplicate_vehicle_data(existing_records, new_entry):\n",
    "    try:\n",
    "        for record in existing_records:\n",
    "            if record.get(\"listing_url\") == new_entry.get(\"listing_url\"): #and record.get(\"title\") == new_entry.get(\"title\")\n",
    "                # Raise an exception to log the error for a duplicate record\n",
    "                raise ValueError(f\"Duplicate found:  URL: {new_entry.get('listing_url')}\")\n",
    "        return False\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error duplicate data: {ve}\")\n",
    "        slack_obj.send_message(\n",
    "            message=f\"Error‚ùå: \"\n",
    "                    f\"Error Duplicate Vehicle data\"\n",
    "                    f\"URL: {new_entry.get('listing_url')}, \"\n",
    "                    f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, \"\n",
    "                    f\"ERROR: {str(ve)\n",
    "                    }\",\n",
    "            channel_id=\"C08FQ51LEAF\"\n",
    "        )\n",
    "        return True"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. trigger extraction jobs for all dealerships and\n",
    "**Problem statement**\n",
    "\n",
    "we need to query our database for all available dealerhips, determine what marketplace they post their vehicles in, and trigger the extraction job.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Additional Retry attempts (2)\n",
    "given a failed vehicle scrape, the system should attempt to retry 2 additional times. if after the 3 consecutive attempts we were still unable to complete the job, notify the failure via paper boy"
   ]
  },
  {
   "metadata": {
    "id": "ql3J2kpNS1sv",
    "ExecuteTime": {
     "end_time": "2025-03-27T14:31:02.294302Z",
     "start_time": "2025-03-27T14:30:58.017725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "API_URL = \"https://retoolapi.dev/OQm51O/data\"  # Replace with your actual POST URL\n",
    "CSV_FILE = \"cargurus_cars_data.csv\"\n",
    "SEND_BATCH_SIZE = 1  # Number of records to send in each batch\n",
    "MAX_RETRY_ATTEMPTS = 2  # Maximum number of retry attempts for a failed batch\n",
    "\n",
    "# # DEMO Sample data to post\n",
    "# vehicle_output_data_demo = [\n",
    "#     {'inventory_source_id': '323e4567-e89b-12d3-a456-426614174002', 'listing_url': 'https://www.cargurus.com/Cars/m-Twins-Auto-Sales--Taylor-sp457133#listing=408360065/NONE/DEFAULT', 'status': 'available', 'price': 9995.0, 'vehicle': {'dealership_id': '123e4567-e89b-12d3-a456-426614174000', 'vin': '5XYPGDA51HG315228', 'mileage': 115771, 'stock_number': '441', 'description': '', 'exterior_color': 'Gray', 'interior_color': 'Gray', 'model': {'name': 'Sorento', 'year': '2017', 'trim': 'LX V6 AWD', 'body_style': 'SUV / Crossover', 'transmission': '6-Speed Automatic', 'fuel_type': 'Gasoline', 'drivetrain': 'All-Wheel Drive', 'engine': '290 hp 3.3L V6', 'make': {'name': 'Kia'}}}\n",
    "#      }\n",
    "# ]\n",
    "def read_csv_file(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the data from the CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # if df.empty():\n",
    "        #     raise FileNotFoundError(f\"Error: File {csv_file} not found.\")\n",
    "        # Convert DataFrame to a list of dictionaries\n",
    "        vehicle_output_data = df.to_dict(orient=\"records\")\n",
    "        print(f\"Successfully read {len(df)} records from {csv_file}\")\n",
    "        return vehicle_output_data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {csv_file} not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_and_send_batches_by_post_data(API_URL, vehicle_output_data, SEND_BATCH_SIZE):\n",
    "    # Fetch existing records from GET_URL\n",
    "    existing_records = get_vehicle_data(API_URL)\n",
    "\n",
    "    # Filter out duplicates\n",
    "    non_duplicate_data = [item for item in vehicle_output_data if not is_duplicate_vehicle_data(existing_records, item)]\n",
    "    print(f\"Filtered {len(vehicle_output_data) - len(non_duplicate_data)} duplicate records.\")\n",
    "\n",
    "    # non_duplicate_data = [item for item in vehicle_output_data]\n",
    "    # Split data into batches\n",
    "    # total_records = len(vehicle_output_data)\n",
    "    total_records = len(non_duplicate_data)\n",
    "    total_batches = ceil(total_records / SEND_BATCH_SIZE)\n",
    "    # print(total_records)\n",
    "    for batch_number in range(total_batches):\n",
    "        # Get the current batch\n",
    "        start_idx = batch_number * SEND_BATCH_SIZE\n",
    "        end_idx = min(start_idx + SEND_BATCH_SIZE, total_records) #start_idx + SEND_BATCH_SIZE\n",
    "        # data_batch = vehicle_output_data[start_idx:end_idx][0]\n",
    "        data_batch = non_duplicate_data[start_idx:end_idx][0]\n",
    "        # print(data_batch)\n",
    "\n",
    "        print(f\"Sending Batch {batch_number + 1}/{total_batches} (Records {start_idx + 1}-{min(end_idx, total_records)})...\")\n",
    "\n",
    "        # Attempt to send the batch with retry\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        while attempt < MAX_RETRY_ATTEMPTS and not success:\n",
    "            success = post_vehicle_data(API_URL, data_batch)\n",
    "            attempt += 1\n",
    "            if not success:\n",
    "                print(f\"Retrying... Attempt {attempt}/{MAX_RETRY_ATTEMPTS}\")\n",
    "                time.sleep(2)  # Optional sleep between retry attempts\n",
    "                # continue\n",
    "\n",
    "        # Handle a permanently failed batch\n",
    "        if not success:\n",
    "            print(f\"Batch {batch_number + 1} failed after {MAX_RETRY_ATTEMPTS} attempts.\")\n",
    "\n",
    "# csv file data transfer\n",
    "vehicle_output_data_csv = read_csv_file(CSV_FILE)\n",
    "process_and_send_batches_by_post_data(\n",
    "    API_URL,\n",
    "    vehicle_output_data_csv,\n",
    "    SEND_BATCH_SIZE)\n",
    "\n",
    "# Define the job to run\n",
    "# def job():\n",
    "#     print(\"Starting the data send job...\")\n",
    "#     process_and_send_batches_by_post_data(\n",
    "#     API_URL,\n",
    "#     vehicle_output_data_csv,\n",
    "#     SEND_BATCH_SIZE)\n",
    "#     print(\"Job completed.\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 12 records from cargurus_cars_data.csv\n",
      "Successfully get records. Status: 200\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=409458789/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=412081215/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=411540591/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=412194769/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=381943807/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=371632943/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=406931063/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=402785142/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=402005545/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=398076886/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=412081216/NONE/DEFAULT\n",
      "Error duplicate data: Duplicate found:  URL: https://www.cargurus.com/Cars/m-Rogers-Auto-Sales-Inc-sp340683#listing=409911245/NONE/DEFAULT\n",
      "Filtered 12 duplicate records.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Scraper Cron Job implementation\n",
    "create a mechanism that triggers new scrapes to happen every 72 hours for all our registered dealerships.\n",
    "1. Open the crontab to edit it:\n",
    "``` bash\n",
    "   crontab -e\n",
    "```\n",
    "1. Add the following line to schedule the job:\n",
    "``` bash\n",
    "   0 */3 * * * /usr/bin/python3 /path/to/send_vehicle_data.py >> /path/to/logfile.log 2>&1\n",
    "```\n",
    "- `0 */3 * * *` schedules the script to run every 3 hours on the hour (e.g., 0:00, 3:00, 6:00, etc.).\n",
    "- `/usr/bin/python3` is the path to your Python 3 executable.\n",
    "- `/path/to/send_vehicle_data.py` is the full path to your Python script.\n",
    "- `>> /path/to/logfile.log 2>&1` redirects output and errors to a log file for debugging purposes.\n",
    "\n",
    "1. Save and exit.\n",
    "2. Check that the cron job is active:\n",
    "``` bash\n",
    "   crontab -l\n",
    "```\n",
    "1. Ensure the cron service is running on your server:\n",
    "``` bash\n",
    "   sudo service cron start\n",
    "```\n",
    "### 3. **Logging Output (Optional)**\n",
    "To monitor the script's behavior, you can redirect the output of the script to a log file in the cron job. For example:\n",
    "``` bash\n",
    "0 */3 * * * /usr/bin/python3 /path/to/send_vehicle_data.py >> /path/to/logs/send_vehicle_data.log 2>&1\n",
    "```\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the job to run\n",
    "# def job():\n",
    "#     print(\"Starting the data send job...\")\n",
    "#     vehicle_output_data_csv = read_csv_file(CSV_FILE)\n",
    "#     process_and_send_batches_by_post_data(API_URL,vehicle_output_data_csv,SEND_BATCH_SIZE)\n",
    "#     print(\"Job completed.\")\n",
    "#\n",
    "#\n",
    "# # Schedule the task\n",
    "# SCHEDULE_INTERVAL_MINUTES = 2\n",
    "# schedule.every(SCHEDULE_INTERVAL_MINUTES).minutes.do(job)\n",
    "#\n",
    "#\n",
    "# # Keep the script running to execute jobs\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 12. Deploy Scraper to server to digital Ocean"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
